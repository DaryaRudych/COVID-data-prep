{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import geopy\n",
    "from time import sleep\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import pycountry\n",
    "import os\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#os.getcwd() - code to get current working directory\n",
    "# import requests\n",
    "# import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_geocode(address):\n",
    "    \"\"\"\n",
    "    create geopy class instance\n",
    "    and path address to get latitude, longitude\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "    try:\n",
    "        lat = geocode(address).latitude\n",
    "        long = geocode(address).longitude\n",
    "        return lat, long\n",
    "    except GeocoderTimedOut:\n",
    "        return do_geocode(address)\n",
    "\n",
    "def get_country_code(country):\n",
    "    \"\"\"\n",
    "    create pycountry instance and pass country name to get country code\n",
    "    \"\"\"\n",
    "    mapping = {country.name: country.alpha_2 for country in pycountry.countries}\n",
    "    return mapping.get(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_dir - path to root directory\n",
    "#realms_file - string name of file with realms data (i.e. \"realms.csv\")\n",
    "#new_file - string name for new file that will be created (i.e. \"clean.csv\")\n",
    "\n",
    "def clean_data(base_dir, realms_file, new_file):\n",
    "    \"\"\"\n",
    "    function to fetch, clean and save COVID-19 data\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://raw.githubusercontent.com/datasets/covid-19/master/data/time-series-19-covid-combined.csv\"\n",
    "\n",
    "    # read data into pandas df and do some renaming&cleaning\n",
    "    data = pd.read_csv(url, error_bad_lines=False, parse_dates=['Date'])\n",
    "    data = data.rename(columns={'Country/Region': 'Country'})\n",
    "    data = data[data.Country != 'Cruise Ship']\n",
    "\n",
    "    #Create empty list to which we will append countries with inconsistent coordinates\n",
    "\n",
    "\n",
    "    #Create new df column and populate with some datetime value that we will replace\n",
    "    data['virus_start_dt'] = pd.to_datetime(data['Date'].min())\n",
    "\n",
    "    for i in data['Country'].unique():\n",
    "        country_name = i\n",
    "        if data['Confirmed'].loc[(data['Country']==country_name)].sum()== 0:\n",
    "            data['virus_start_dt'].loc[(data['Country'] == country_name)] = np.NaN\n",
    "        elif data['Confirmed'].loc[(data['Country']==country_name)].sum()> 0:\n",
    "            virus_start_dt = data.loc[(data['Country']==country_name)&(data['Confirmed']>=1)].groupby('Country')['Date'].min()\n",
    "            data['virus_start_dt'].loc[(data['Country'] == country_name)] = virus_start_dt[0]\n",
    "    countries = []\n",
    "    for i in data.groupby('Country').apply(lambda x: x['Country'].unique()):\n",
    "        #Check and store countries that have more than one province in original dataset\n",
    "        if data['Province/State'].loc[(data['Country']==i[0])].nunique()>1:\n",
    "            countries.append(i[0])\n",
    "        if i in countries:\n",
    "            country_name = i[0]\n",
    "            lat,long = do_geocode(country_name) #get new coordinates from geopy\n",
    "            data['Lat'].loc[(data['Country'] == country_name)]=lat # update latitude\n",
    "            data['Long'].loc[(data['Country'] == country_name)]=long #update longitude  \n",
    "    \n",
    "    # Aggregate clean data\n",
    "    aggs = {'Confirmed': 'max', 'Deaths': 'max', 'Date':'max'}\n",
    "    data = data.groupby(by=['Country', 'Lat', 'Long', 'virus_start_dt' ],  as_index=False).agg(aggs)\n",
    "    print(data)\n",
    "    data = data.rename(columns={'Date': 'last_upd'})\n",
    "    data\n",
    "\n",
    "    name = []\n",
    "    for i in pycountry.countries:\n",
    "        if hasattr (i, \"name\"):\n",
    "            name.append(i.name)\n",
    "        elif hasattr (i, \"common_name\"):\n",
    "            name.append(i.common_name)\n",
    "        elif hasattr (i, \"official_name\"):\n",
    "            name.append(i.official_name)\n",
    "    invalid = ([i for i in  data['Country'].unique() if(i not in name)])\n",
    "    for i in invalid:\n",
    "        pattern = re.compile(i)\n",
    "        result = [j for j in name if pattern.match(j)]\n",
    "        if result:\n",
    "            data['Country'].loc[(data['Country'] == i)]=result[0]\n",
    "\n",
    "    data['Country'].loc[(data['Country'] == \"US\")]='United States'\n",
    "    #For each unique country in df get alpha 2 country code from pycountry \n",
    "    data['country_code'] = \"\"\n",
    "    for i in data['Country']:\n",
    "        data['country_code'].loc[(data['Country'] == i)] = get_country_code(i)\n",
    "\n",
    "    #Hard-code country code for Congo\n",
    "    data['country_code'].loc[(data['Country'] == \"Congo (Brazzaville)\")] = \"CG\"\n",
    "    data['country_code'].loc[(data['Country'] == \"Congo (Kinshasa)\")] = \"CD\"\n",
    "\n",
    "    #Read csv file with dw_pr countries and corresponding realms and codes into pandas df\n",
    "    fullpath = os.path.join(base_dir, realms_file)\n",
    "    realms = pd.read_csv(fullpath)\n",
    "\n",
    "    #Grab two columns\n",
    "    realms = realms[['ISO_ALPH2', 'WG_REGION']]\n",
    "\n",
    "    #Rename column ISO_ALPH2 to match same column in data df\n",
    "    realms = realms.rename(columns={'ISO_ALPH2': 'country_code'})\n",
    "\n",
    "    #Merge two dfs on common column = country_code\n",
    "    clean = pd.merge(data, realms, on='country_code', how='left')\n",
    "\n",
    "    #Realm \"NA\" gets in interpreted as NaN, fix that\n",
    "    clean['WG_REGION'].fillna(\"NA\", inplace=True)\n",
    "\n",
    "    #Create path to where output will be saved\n",
    "    new_path = os.path.join(base_dir, new_file)\n",
    "\n",
    "    #Return cleaned combined data\n",
    "    return clean.to_csv(new_path, index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Country      Lat      Long virus_start_dt  Confirmed  Deaths  \\\n",
      "0           Afghanistan  33.0000   65.0000     2020-02-24      174.0     4.0   \n",
      "1               Albania  41.1533   20.1683     2020-03-09      243.0    15.0   \n",
      "2               Algeria  28.0339    1.6596     2020-02-25      716.0    44.0   \n",
      "3               Andorra  42.5063    1.5218     2020-03-02      376.0    12.0   \n",
      "4                Angola -11.2027   17.8739     2020-03-20        7.0     2.0   \n",
      "..                  ...      ...       ...            ...        ...     ...   \n",
      "175           Venezuela   6.4238  -66.5897     2020-03-14      135.0     3.0   \n",
      "176             Vietnam  16.0000  108.0000     2020-01-23      212.0     0.0   \n",
      "177  West Bank and Gaza  31.9522   35.2332     2020-03-05      119.0     1.0   \n",
      "178              Zambia -15.4167   28.2833     2020-03-18       35.0     0.0   \n",
      "179            Zimbabwe -20.0000   30.0000     2020-03-20        8.0     1.0   \n",
      "\n",
      "          Date  \n",
      "0   2020-03-31  \n",
      "1   2020-03-31  \n",
      "2   2020-03-31  \n",
      "3   2020-03-31  \n",
      "4   2020-03-31  \n",
      "..         ...  \n",
      "175 2020-03-31  \n",
      "176 2020-03-31  \n",
      "177 2020-03-31  \n",
      "178 2020-03-31  \n",
      "179 2020-03-31  \n",
      "\n",
      "[180 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "realms_file = \"realms.csv\"\n",
    "new_file = \"cleaned.csv\"\n",
    "\n",
    "clean_data(base_dir, realms_file, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "realms_file = \"realms.csv\"\n",
    "new_file = \"timeseries.csv\"\n",
    "url = \"https://raw.githubusercontent.com/datasets/covid-19/master/data/time-series-19-covid-combined.csv\"\n",
    "\n",
    "# read data into pandas df and do some renaming&cleaning\n",
    "data = pd.read_csv(url, error_bad_lines=False, parse_dates=['Date'])\n",
    "data = data.rename(columns={'Country/Region': 'Country'})\n",
    "data = data[data.Country != 'Cruise Ship']\n",
    "\n",
    "\n",
    "#Create new df column and populate with some datetime value that we will replace\n",
    "data['virus_start_dt'] = pd.to_datetime(data['Date'].min())\n",
    "\n",
    "for i in data['Country'].unique():\n",
    "    country_name = i\n",
    "    if data['Confirmed'].loc[(data['Country']==country_name)].sum()== 0:\n",
    "        data['virus_start_dt'].loc[(data['Country'] == country_name)] = np.NaN\n",
    "    elif data['Confirmed'].loc[(data['Country']==country_name)].sum()> 0:\n",
    "        virus_start_dt = data.loc[(data['Country']==country_name)&(data['Confirmed']>=1)].groupby('Country')['Date'].min()\n",
    "        data['virus_start_dt'].loc[(data['Country'] == country_name)] = virus_start_dt[0]\n",
    "\n",
    "#Create empty list to which we will append countries with inconsistent coordinates\n",
    "countries = []\n",
    "for i in data.groupby('Country').apply(lambda x: x['Country'].unique()):\n",
    "    #Check and store countries that have more than one province in original dataset\n",
    "    if data['Province/State'].loc[(data['Country']==i[0])].nunique()>1:\n",
    "        countries.append(i[0])\n",
    "    if i in countries:\n",
    "        country_name = i[0]\n",
    "        lat,long = do_geocode(country_name) #get new coordinates from geopy\n",
    "        data['Lat'].loc[(data['Country'] == country_name)]=lat # update latitude\n",
    "        data['Long'].loc[(data['Country'] == country_name)]=long #update longitude  \n",
    "            \n",
    "for i, j in data.iterrows():\n",
    "    if j['virus_start_dt']> j['Date']:\n",
    "        data.drop(i, inplace=True)\n",
    "\n",
    "\n",
    "#Aggregate clean data by country  \n",
    "aggs = {'Confirmed': 'sum', 'Deaths': 'sum', }\n",
    "data = data.groupby(by=['Date', 'Country', 'Lat', 'Long', 'virus_start_dt' ],  as_index=False).agg(aggs)\n",
    "data = data.rename(columns={'Date': 'last_upd'})\n",
    "name = []\n",
    "\n",
    "for i in pycountry.countries:\n",
    "    if hasattr (i, \"name\"):\n",
    "        name.append(i.name)\n",
    "    elif hasattr (i, \"common_name\"):\n",
    "        name.append(i.common_name)\n",
    "    elif hasattr (i, \"official_name\"):\n",
    "        name.append(i.official_name)\n",
    "invalid = ([i for i in  data['Country'].unique() if(i not in name)])\n",
    "for i in invalid:\n",
    "    pattern = re.compile(i)\n",
    "    result = [j for j in name if pattern.match(j)]\n",
    "    if result:\n",
    "        data['Country'].loc[(data['Country'] == i)]=result[0]\n",
    "\n",
    "data['Country'].loc[(data['Country'] == \"US\")]='United States'\n",
    "#For each unique country in df get alpha 2 country code from pycountry \n",
    "data['country_code'] = \"\"\n",
    "\n",
    "for i in data['Country']:\n",
    "    data['country_code'].loc[(data['Country'] == i)] = get_country_code(i)\n",
    "\n",
    "#Hard-code country code for Congo\n",
    "data['country_code'].loc[(data['Country'] == \"Congo (Brazzaville)\")] = \"CG\"\n",
    "data['country_code'].loc[(data['Country'] == \"Congo (Kinshasa)\")] = \"CD\"\n",
    "\n",
    "#Read csv file with dw_pr countries and corresponding realms and codes into pandas df\n",
    "fullpath = os.path.join(base_dir, realms_file)\n",
    "realms = pd.read_csv(fullpath)\n",
    "\n",
    "#Grab two columns\n",
    "realms = realms[['ISO_ALPH2', 'WG_REGION']]\n",
    "\n",
    "#   #Rename column ISO_ALPH2 to match same column in data df\n",
    "realms = realms.rename(columns={'ISO_ALPH2': 'country_code'})\n",
    "\n",
    "#Merge two dfs on common column = country_code\n",
    "clean = pd.merge(data, realms, on='country_code', how='left')\n",
    "\n",
    "#Realm \"NA\" gets in interpreted as NaN, fix that\n",
    "clean['WG_REGION'].fillna(\"NA\", inplace=True)\n",
    "\n",
    "#Create path to where output will be saved\n",
    "new_path = os.path.join(base_dir, new_file)\n",
    "\n",
    "#Return cleaned combined data\n",
    "clean.to_csv(new_path, index = False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
